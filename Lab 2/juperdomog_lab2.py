# -*- coding: utf-8 -*-
"""juperdomog_Lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sN8UQGNXMLotvVdkdu_kVI89MfqRzofB
"""

!pip install -U fortran-magic

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %load_ext fortranmagic

import sys; sys.path.append('..')

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

mpl.rc('figure', figsize=(12, 7))

ran_the_first_cell = True

jan2017 = pd.to_datetime(['2017-01-03 00:00:00+00:00',
 '2017-01-04 00:00:00+00:00',
 '2017-01-05 00:00:00+00:00',
 '2017-01-06 00:00:00+00:00',
 '2017-01-09 00:00:00+00:00',
 '2017-01-10 00:00:00+00:00',
 '2017-01-11 00:00:00+00:00',
 '2017-01-12 00:00:00+00:00',
 '2017-01-13 00:00:00+00:00',
 '2017-01-17 00:00:00+00:00',
 '2017-01-18 00:00:00+00:00',
 '2017-01-19 00:00:00+00:00',
 '2017-01-20 00:00:00+00:00',
 '2017-01-23 00:00:00+00:00',
 '2017-01-24 00:00:00+00:00',
 '2017-01-25 00:00:00+00:00',
 '2017-01-26 00:00:00+00:00',
 '2017-01-27 00:00:00+00:00',
 '2017-01-30 00:00:00+00:00',
 '2017-01-31 00:00:00+00:00',
 '2017-02-01 00:00:00+00:00'])
calendar = jan2017.values.astype('datetime64[D]')

event_dates = pd.to_datetime(['2017-01-06 00:00:00+00:00', 
                             '2017-01-07 00:00:00+00:00', 
                             '2017-01-08 00:00:00+00:00']).values.astype('datetime64[D]')
event_values = np.array([10, 15, 20])

"""<center>
  <h1>The PyData Toolbox</h1>
  <h3>Scott Sanderson (Twitter: @scottbsanderson, GitHub: ssanderson)</h3>
  <h3><a href="https://github.com/ssanderson/pydata-toolbox">https://github.com/ssanderson/pydata-toolbox</a></h3>
</center>

# About Me:

<img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/me.jpg" alt="Drawing" style="width: 300px;"/>

- Senior Engineer at [Quantopian](www.quantopian.com)
- Background in Mathematics and Philosophy
- **Twitter:** [@scottbsanderson](https://twitter.com/scottbsanderson)
- **GitHub:** [ssanderson](github.com/ssanderson)

## Outline

- Built-in Data Structures
- Numpy `array`
- Pandas `Series`/`DataFrame`
- Plotting and "Real-World" Analyses

# Data Structures

> Rule 5. Data dominates. If you've chosen the right data structures and organized things well, the algorithms
will almost always be self-evident. Data structures, not algorithms, are central to programming.

- *Notes on Programming in C*, by Rob Pike.

# Lists
"""

assert ran_the_first_cell, "Oh noes!"

l = [1, 'two', 3.0, 4, 5.0, "six"]
l

# Lists can be indexed like C-style arrays.
first = l[0]
second = l[1]
print("first:", first)
print("second:", second)

# Negative indexing gives elements relative to the end of the list.
last = l[-1]
penultimate = l[-2]
print("last:", last)
print("second to last:", penultimate)

# Lists can also be sliced, which makes a copy of elements between 
# start (inclusive) and stop (exclusive)
sublist = l[1:3]
sublist

# l[:N] is equivalent to l[0:N].
first_three = l[:3]
first_three

# l[3:] is equivalent to l[3:len(l)].
after_three = l[3:]
after_three

# There's also a third parameter, "step", which gets every Nth element.
l = ['a', 'b', 'c', 'd', 'e', 'f', 'g','h']
l[1:7:2]

# This is a cute way to reverse a list.
l[::-1]

# Lists can be grown efficiently (in O(1) amortized time).
l = [1, 2, 3, 4, 5]
print("Before:", l)
l.append('six')
print("After:", l)

# Comprehensions let us perform elementwise computations.
l = [1, 2, 3, 4, 5]
[x * 2 for x in l]

"""## Review: Python Lists

- Zero-indexed sequence of arbitrary Python values.
- Slicing syntax: `l[start:stop:step]` copies elements at regular intervals from `start` to `stop`.
- Efficient (`O(1)`) appends and removes from end.
- Comprehension syntax: `[f(x) for x in l if cond(x)]`.

# Dictionaries
"""

# Dictionaries are key-value mappings.
philosophers = {'David': 'Hume', 'Immanuel': 'Kant', 'Bertrand': 'Russell'}
philosophers

# Like lists, dictionaries are size-mutable.
philosophers['Ludwig'] = 'Wittgenstein'
philosophers

del philosophers['David']
philosophers

# No slicing.
philosophers['Bertrand':'Immanuel']

"""## Review: Python Dictionaries

- Unordered key-value mapping from (almost) arbitrary keys to arbitrary values.
- Efficient (`O(1)`) lookup, insertion, and deletion.
- No slicing (would require a notion of order).

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/pacino.gif" alt="Drawing" style="width: 100%;"/></center>
"""

# Suppose we have some matrices...
a = [[1, 2, 3],
     [2, 3, 4],
     [5, 6, 7],
     [1, 1, 1]]

b = [[1, 2, 3, 4],
     [2, 3, 4, 5]]

def matmul(A, B):
    """Multiply matrix A by matrix B."""
    rows_out = len(A)
    cols_out = len(B[0])
    out = [[0 for col in range(cols_out)] for row in range(rows_out)]
    
    for i in range(rows_out):
        for j in range(cols_out):
            for k in range(len(B)):
                out[i][j] += A[i][k] * B[k][j]
    return out

"""<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/gross.gif" alt="Drawing" style="width: 50%;"/></center>

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# matmul(a, b)

"""**My own example 0 - cpu info**"""

!cat /proc/cpuinfo

"""**My own example 1 - Changing in matmul(A, B) Python len(B) (# of rows of B) for len(A[0]) (# of columns of A)**"""

def matmul(A, B):
    """Multiply matrix A by matrix B."""
    rows_out = len(A)
    cols_out = len(B[0])
    out = [[0 for col in range(cols_out)] for row in range(rows_out)]
    
    for i in range(rows_out):
        for j in range(cols_out):
            for k in range(len(A[0])):
                out[i][j] += A[i][k] * B[k][j]
    return out

"""**My own example 2 - Verifiying error with in matmul(A, B) Python with the original matrices when changing len(B) (# of rows of B) for len(A[0]) (# of colums of A)**"""

matmul(a,b)

"""**My own example 3 - Chekcing the matrix multiplication compatibility condition  len(A[0]) == len(B)**"""

def matmul(A, B):
    """Multiply matrix A by matrix B."""
    if len(A[0]) == len(B):
      rows_out = len(A)
      cols_out = len(B[0])
      out = [[0 for col in range(cols_out)] for row in range(rows_out)]
    
      for i in range(rows_out):
          for j in range(cols_out):
              for k in range(len(A[0])):
                out[i][j] += A[i][k] * B[k][j]
      return out
    else:
      print('This multiplication is not posible')

"""**My own example 4 -  Verifiying error with in matmul(A, B) Python when checking the mtarix multiplication compatibility condition  len(A[0]) == len(B)**"""

matmul(a,b)

"""**My own example 5 - Deifining A and B that are compatible for multiplcation**"""

A = [[3,6,8,4],
     [5,6,1,2],
     [3,1,0,2],
     [2,5,7,8]]
B = [[3,5],
     [2,5],
     [3,8],
     [8,4]]    
matmul(A,B)

"""**My own example 6 - Runinng the correct Python matrix multiplication code with the matrices with dimensions compatible for multiplication.**"""

import random

random.normalvariate(0,1)

import random
def random_matrix(m, n):
    out = []
    for row in range(m):
        out.append([random.random() for _ in range(n)])
    return out

randm = random_matrix(2, 3)
randm

"""**My own example 7 - Running 10 times matmul(randa, randb) with randa and randb a randon matrices of 600 x 100 and 100 x 600 and calulating the average execution time**"""

import time
randa = random_matrix(600,100)
randb = random_matrix(100,600)
sumatory = 0
for i in range(10):
  start = time.process_time()
  matmul(randa,randb)
  end = time.process_time()
  sumatory += end - start
average = sumatory/10
print(average)

"""**My own example 8 - Creating the average execution time data frame and adding Python's average execution time**"""

dict = {'Language':['Phyton'], 'Average execution time':[average],}
T = pd.DataFrame(dict)
T

"""**My own example 9 - Running 10 times randa and randb mutiplicaction as NumPy arrays  adding NumPy's average execution time**"""

import time
randanp = np.array(randa)
randbnp = np.array(randb)
sumatorynp = 0
for i in range(10):
  start = time.process_time()
  randanp.dot(randbnp)
  end = time.process_time()
  sumatorynp += end - start
averagenp = sumatorynp / 10

T.loc[len(T.index)] = ['Numpy',averagenp]
T

# Maybe that's not that bad?  Let's try a simpler case.
def python_dot_product(xs, ys):
    return sum(x * y for x, y in zip(xs, ys))

# Commented out IPython magic to ensure Python compatibility.
# %%fortran
# subroutine fortran_dot_product(xs, ys, result)
#     double precision, intent(in) :: xs(:)
#     double precision, intent(in) :: ys(:)
#     double precision, intent(out) :: result
#     
#     result = sum(xs * ys)
# end

list_data = [float(i) for i in range(100000)]
array_data = np.array(list_data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# python_dot_product(list_data, list_data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# fortran_dot_product(array_data, array_data)

"""<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/sloth.gif" alt="Drawing" style="width: 1080px;"/></center>

**My own example 10 - Deifining A (2x2)  and B (2x2)**
"""

a = [[1,6],
     [5,7]]
b = [[5,8],
     [6,2]]

"""**My own example 11 - Defining Fortran subroutine matmul(A,B) for 2x2 matrices**"""

# Commented out IPython magic to ensure Python compatibility.
# %%fortran
# subroutine fortran_matmul(A, B, result)
#     double precision, intent(in) :: A(2,2)
#     double precision, intent(in) :: B(2,2)
#     double precision, intent(out) :: result(2,2)
#     
#     result = matmul(A,B)
# end

"""**My own example 12 -Run Fortran subroutine matmul(A,B) with a and b 2x2 matrices**"""

fortran_matmul(a,b)

"""**My own example 13 - Defining Fortran subroutine matmul(A,B) for 600x100 and 100x600 matrices**"""

# Commented out IPython magic to ensure Python compatibility.
# %%fortran
# subroutine fortran_matmul(A, B, result)
#     double precision, intent(in) :: A(600,100)
#     double precision, intent(in) :: B(100,600)
#     double precision, intent(out) :: result(600,600)
#     
#     result = matmul(A,B)
# end

"""**My own example 14 -Run Fortran subroutine matmul(A,B) with 600x100 and 100x600 matrices**"""

fortran_matmul(randa,randb)

"""**My own example 15 - Running 10 times the  Fortran subroutine matmul(A,B) with 600x100 and 100x600 matrices and adding Fortran magic average execution time to the data frame**"""

import time
sumatoryft = 0
for i in range(10):
  start = time.process_time()
  fortran_matmul(randa,randb)
  end = time.process_time()
  sumatoryft += end - start
averageft = sumatoryft / 10

T.loc[len(T.index)] = ['Fortranmagic',averageft]
T

"""**My own example 16 - Creating a  Fortran program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile tmpf.f
#       program matrix multiplis
# 
#         double precision, dimension (600,100) :: A
#         double precision, dimension (100,600):: B
#         double precision, dimension (600,600):: result
#         integer :: i, j,  r
#         real :: sumatoryf,start,end,averagef
#     
#         do i = 1, 600
#             do j = 1, 100
#               A(i, j) = i+j*0.5
#             end do
#         end do
#    
#         do i = 1, 100
#             do j = 1, 600
#               B(i, j) = i+j*0.5
#             end do
#         end do
#         sumatoryf = 0
#         averagef = 0
# 
#         do r = 1,10
#               call cpu_time(start)
#               result = matmul(A, B)
#               call cpu_time(end)
#               sumatoryf = sumatoryf + (end - start)
#         end do
#         averagef = sumatoryf / 10
#         PRINT *,averagef
#         stop
#       end

"""**My own example 17 - Running the Fortran program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

!gfortran tmpf.f -o tmpf
!./tmpf

"""**My own example 18 - Adding Fortran average execution time to the data frame**"""

T.loc[len(T.index)] = ['Fortran',7.75940111E-03]
T

"""**My own example 19 - Creating a c program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile tmpc.c
# 
# #include <stdio.h>
# #include <time.h>
# 
# 
# int main(){
# 
#   int A[600][100];
#   int B[100][600];
#   int result[600][600];
#   double sumatoryc = 0; 
#   for (int test = 0; test < 10; test++){
#    
#     clock_t start = clock();
# 
#     for (int c = 0; c < 600; c++){
#           for (int f = 0; f < 600; f++){
#               int suma = 0;
#               for (int j = 0; j < 100; j++){
#                   suma += A[f][j] * B[j][c];
#               }
#               result[c][f] = suma;
#           }
#       }
#       clock_t end = clock();
#       sumatoryc += (double)(end - start) / CLOCKS_PER_SEC;
#     }
#     double averagec = sumatoryc/10;
#     printf("%f\n",averagec);
#    
# 
#   return 0;
# }

"""**My own example 20 - Running the c program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

!gcc tmpc.c -o tmpc
!./tmpc

"""**My own example 21 - Adding c average execution time to the data frame**"""

T.loc[len(T.index)] = ['C',0.129232]
T

"""**My own example 22 - Creating a C++ program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile tmpcc.cc
# #include <iostream>
# #include <ctime>
# using namespace std;
# 
# int main() {
#   int A[600][100];
#   int B[100][600];
#   int result[600][600];
#   double sumatoryc = 0; 
#   for (int test = 0; test < 10; test++){
#    
#     clock_t start = clock();
# 
#     for (int c = 0; c < 600; c++){
#           for (int f = 0; f < 600; f++){
#               int suma = 0;
#               for (int j = 0; j < 100; j++){
#                   suma += A[f][j] * B[j][c];
#               }
#               result[c][f] = suma;
#           }
#       }
#       clock_t end = clock();
#       sumatoryc += (double)(end - start) / CLOCKS_PER_SEC;
#     }
#     double averagec = sumatoryc/10;
#     printf("%f\n",averagec);  
# 	return 0;
# }

"""**My own example 23 - Running the C++ program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

!g++ tmpcc.cc -o tmpcc
!./tmpcc

"""**My own example 24 - Adding C++ average execution time to the data frame**"""

T.loc[len(T.index)] = ['C++',0.116362]
T

"""**My own example 25 - Creating a Java program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile Main.java
# 
# class Main {
# 
#   public static void main(String[] args) {
#     int [][] A = new int[600][100];
#     int [][] B = new int[100][600];
#     int [][] result = new int[600][600];
#     double sumatoryc = 0; 
#     for (int test = 0; test < 10; test++){
#     
#       long start = System.currentTimeMillis();
# 
#       for (int c = 0; c < 600; c++){
#             for (int f = 0; f < 600; f++){
#                 int suma = 0;
#                 for (int j = 0; j < 100; j++){
#                     suma += A[f][j] * B[j][c];
#                 }
#                 result[c][f] = suma;
#             }
#         }
#         long end = System.currentTimeMillis();
#         sumatoryc += (double)(end - start) / 1000;
#       }
#       double averagec = sumatoryc/10;
#       System.out.printf("%f\n",averagec);  
#     
#   }
# }

"""**My own example 26 - Running the Java program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""

!javac Main.java
!java Main

"""**My own example 27 - Adding Java average execution time to the data frame**"""

T.loc[len(T.index)] = ['Java',0.065800]
T

"""**My own example 28 - Creating a Javascript program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""



"""**My own example 29 - Running the Javascript program that mutiplies 10 times A(600x100) and  B (100x600) matrices**"""



"""**My own example 30 - Adding Javascript average execution time to the data frame**"""



"""**My own example 31 - Finding the minimun average esecuiton time in the data frame**"""

mins = T['Average execution time'].min()
print(mins)

"""**My own example 32 - Adding the Speed factor columne to the data frame**"""

A = T['Average execution time']/mins
T.insert(2,'Speed factor',A,allow_duplicates=False)

"""**My own example 33 - Sorting the the data frame by average execution time**"""

Ta = T.sort_values('Average execution time')
Ta

"""## Why is the Python Version so Much Slower?"""

# Dynamic typing.
def mul_elemwise(xs, ys):
    return [x * y for x, y in zip(xs, ys)]

mul_elemwise([1, 2, 3, 4], [1, 2 + 0j, 3.0, 'four'])
#[type(x) for x in _]

# Interpretation overhead.
source_code = 'a + b * c'
bytecode = compile(source_code, '', 'eval')
import dis; dis.dis(bytecode)

"""## Why is the Python Version so Slow?
- Dynamic typing means that every single operation requires dispatching on the input type.
- Having an interpreter means that every instruction is fetched and dispatched at runtime.
- Other overheads:
  - Arbitrary-size integers.
  - Reference-counted garbage collection.

> This is the paradox that we have to work with when we're doing scientific or numerically-intensive Python. What makes Python fast for development -- this high-level, interpreted, and dynamically-typed aspect of the language -- is exactly what makes it slow for code execution.

- Jake VanderPlas, [*Losing Your Loops: Fast Numerical Computing with NumPy*](https://www.youtube.com/watch?v=EEUXKG97YRw)

# What Do We Do?

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/runaway.gif" alt="Drawing" style="width: 50%;"/></center>

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/thisisfine.gif" alt="Drawing" style="width: 1080px;"/></center>

- Python is slow for numerical computation because it performs dynamic dispatch on every operation we perform...

- ...but often, we just want to do the same thing over and over in a loop!

- If we don't need Python's dynamicism, we don't want to pay (much) for it.

- **Idea:** Dispatch **once per operation** instead of **once per element**.
"""

import numpy as np

data = np.array([1, 2, 3, 4])
data

data + data

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Naive dot product
# (array_data * array_data).sum()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Built-in dot product.
# array_data.dot(array_data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# fortran_dot_product(array_data, array_data)

# Numpy won't allow us to write a string into an int array.
data[0] = "foo"

# We also can't grow an array once it's created.
data.append(3)

# We **can** reshape an array though.
two_by_two = data.reshape(2, 2)
two_by_two

"""Numpy arrays are:

- Fixed-type

- Size-immutable

- Multi-dimensional

- Fast\*

\* If you use them correctly.

# What's in an Array?
"""

arr = np.array([1, 2, 3, 4, 5, 6], dtype='int16').reshape(2, 3)
print("Array:\n", arr, sep='')
print("===========")
print("DType:", arr.dtype)
print("Shape:", arr.shape)
print("Strides:", arr.strides)
print("Data:", arr.data.tobytes())

"""# Core Operations

- Vectorized **ufuncs** for elementwise operations.
- Fancy indexing and masking for selection and filtering.
- Aggregations across axes.
- Broadcasting

# UFuncs

UFuncs (universal functions) are functions that operate elementwise on one or more arrays.
"""

data = np.arange(15).reshape(3, 5)
data

# Binary operators.
data * data

# Unary functions.
np.sqrt(data)

# Comparison operations
(data % 3) == 0

# Boolean combinators.
((data % 2) == 0) & ((data % 3) == 0)

# as of python 3.5, @ is matrix-multiply
data @ data.T

"""# UFuncs Review

- UFuncs provide efficient elementwise operations applied across one or more arrays.
- Arithmetic Operators (`+`, `*`, `/`)
- Comparisons (`==`, `>`, `!=`)
- Boolean Operators (`&`, `|`, `^`)
- Trigonometric Functions (`sin`, `cos`)
- Transcendental Functions (`exp`, `log`)

# Selections

We often want to perform an operation on just a subset of our data.
"""

sines = np.sin(np.linspace(0, 3.14, 10))
cosines = np.cos(np.linspace(0, 3.14, 10))
sines

# Slicing works with the same semantics as Python lists.
sines[0]

sines[:3]  # First three elements

sines[5:]  # Elements from 5 on.

sines[::2]  # Every other element.

# More interesting: we can index with boolean arrays to filter by a predicate.
print("sines:\n", sines)
print("sines > 0.5:\n", sines > 0.5)
print("sines[sines > 0.5]:\n", sines[sines > 0.5])

# We index with lists/arrays of integers to select values at those indices.
print(sines)
sines[[0, 4, 7]]

# Index arrays are often used for sorting one or more arrays.
unsorted_data = np.array([1, 3, 2, 12, -1, 5, 2])

sort_indices = np.argsort(unsorted_data)
sort_indices

unsorted_data[sort_indices]

market_caps = np.array([12, 6, 10, 5, 6])  # Presumably in dollars?
assets = np.array(['A', 'B', 'C', 'D', 'E'])

# Sort assets by market cap by using the permutation that would sort market caps on ``assets``.
sort_by_mcap = np.argsort(market_caps)
assets[sort_by_mcap]

# Indexers are also useful for aligning data.
print("Dates:\n", repr(event_dates))
print("Values:\n", repr(event_values))
print("Calendar:\n", repr(calendar))

print("Raw Dates:", event_dates)
print("Indices:", calendar.searchsorted(event_dates))
print("Forward-Filled Dates:", calendar[calendar.searchsorted(event_dates)])

"""On multi-dimensional arrays, we can slice along each axis independently."""

data = np.arange(25).reshape(5, 5)
data

data[:2, :2]  # First two rows and first two columns.

data[:2, [0, -1]]  # First two rows, first and last columns.

data[(data[:, 0] % 2) == 0]  # Rows where the first column is divisible by two.

"""# Selections Review

- Indexing with an integer removes a dimension.
- Slicing operations work on Numpy arrays the same way they do on lists.
- Indexing with a boolean array filters to True locations.
- Indexing with an integer array selects indices along an axis.
- Multidimensional arrays can apply selections independently along different axes.

## Reductions

Functions that reduce an array to a scalar.

$Var(X) = \frac{1}{N}\sqrt{\sum_{i=1}^N (x_i - \bar{x})^2}$
"""

def variance(x):
    return ((x - x.mean()) ** 2).sum() / len(x)

variance(np.random.standard_normal(1000))

"""- `sum()` and `mean()` are both **reductions**.

- In the simplest case, we use these to reduce an entire array into a single value...
"""

data = np.arange(30)
data.mean()

"""- ...but we can do more interesting things with multi-dimensional arrays."""

data = np.arange(30).reshape(3, 10)
data

data.mean()

data.mean(axis=0)

data.mean(axis=1)

"""## Reductions Review

- Reductions allow us to perform efficient aggregations over arrays.
- We can do aggregations over a single axis to collapse a single dimension.
- Many built-in reductions (`mean`, `sum`, `min`, `max`, `median`, ...).

# Broadcasting
"""

row = np.array([1, 2, 3, 4])
column = np.array([[1], [2], [3]])
print("Row:\n", row, sep='')
print("Column:\n", column, sep='')

row + column

"""<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/broadcasting.png" alt="Drawing" style="width: 60%;"/></center>

<h5>Source: http://www.scipy-lectures.org/_images/numpy_broadcasting.png</h5>
"""

# Broadcasting is particularly useful in conjunction with reductions.
print("Data:\n", data, sep='')
print("Mean:\n", data.mean(axis=0), sep='')
print("Data - Mean:\n", data - data.mean(axis=0), sep='')

"""# Broadcasting Review

- Numpy operations can work on arrays of different dimensions as long as the arrays' shapes are still "compatible".
- Broadcasting works by "tiling" the smaller array along the missing dimension.
- The result of a broadcasted operation is always at least as large in each dimension as the largest array in that dimension.

# Numpy Review

- Numerical algorithms are slow in pure Python because the overhead dynamic dispatch dominates our runtime.

- Numpy solves this problem by:
  1. Imposing additional restrictions on the contents of arrays.
  2. Moving the inner loops of our algorithms into compiled C code.

- Using Numpy effectively often requires reworking an algorithms to use vectorized operations instead of for-loops, but the resulting operations are usually simpler, clearer, and faster than the pure Python equivalent.

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/unicorn.jpg" alt="Drawing" style="width: 75%;"/></center>

Numpy is great for many things, but...

- Sometimes our data is equipped with a natural set of **labels**:
  - Dates/Times
  - Stock Tickers
  - Field Names (e.g. Open/High/Low/Close)

- Sometimes we have **more than one type of data** that we want to keep grouped together.
  - Tables with a mix of real-valued and categorical data.

- Sometimes we have **missing** data, which we need to ignore, fill, or otherwise work around.

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/panda-wrangling.gif" alt="Drawing" style="width: 75%;"/></center>

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/pandas_logo.png" alt="Drawing" style="width: 75%;"/></center>

Pandas extends Numpy with more complex data structures:

- `Series`: 1-dimensional, homogenously-typed, labelled array.
- `DataFrame`: 2-dimensional, semi-homogenous, labelled table.

Pandas also provides many utilities for: 
- Input/Output
- Data Cleaning
- Rolling Algorithms
- Plotting

# Selection in Pandas
"""

s = pd.Series(index=['a', 'b', 'c', 'd', 'e'], data=[1, 2, 3, 4, 5])
s

# There are two pieces to a Series: the index and the values.
print("The index is:", s.index)
print("The values are:", s.values)

# We can look up values out of a Series by position...
s.iloc[0]

# ... or by label.
s.loc['a']

# Slicing works as expected...
s.iloc[:2]

# ...but it works with labels too!
s.loc[:'c']

# Fancy indexing works the same as in numpy.
s.iloc[[0, -1]]

# As does boolean masking.
s.loc[s > 2]

# Element-wise operations are aligned by index.
other_s = pd.Series({'a': 10.0, 'c': 20.0, 'd': 30.0, 'z': 40.0})
other_s

s + other_s

# We can fill in missing values with fillna().
(s + other_s).fillna(0.0)

# Most real datasets are read in from an external file format.
aapl = pd.read_csv('AAPL.csv', parse_dates=['Date'], index_col='Date')
aapl.head()

# Slicing generalizes to two dimensions as you'd expect:
aapl.iloc[:2, :2]

aapl.loc[pd.Timestamp('2010-02-01'):pd.Timestamp('2010-02-04'), ['Close', 'Volume']]

"""# Rolling Operations

<center><img src="https://raw.githubusercontent.com/ssanderson/pydata-toolbox/master/notebooks/images/rolling.gif" alt="Drawing" style="width: 75%;"/></center>
"""

aapl.rolling(5)[['Close', 'Adj Close']].mean().plot();

# Drop `Volume`, since it's way bigger than everything else.
aapl.drop('Volume', axis=1).resample('2W').max().plot();

# 30-day rolling exponentially-weighted stddev of returns.
aapl['Close'].pct_change().ewm(span=30).std().plot();

"""# "Real World" Data"""

from demos.avocados import read_avocadata

avocados = read_avocadata('2014', '2016')
avocados.head()

# Unlike numpy arrays, pandas DataFrames can have a different dtype for each column.
avocados.dtypes

# What's the regional average price of a HASS avocado every day?
hass = avocados[avocados.Variety == 'HASS']
hass.groupby(['Date', 'Region'])['Weighted Avg Price'].mean().unstack().ffill().plot();

def _organic_spread(group):

    if len(group.columns) != 2:
        return pd.Series(index=group.index, data=0.0)
    
    is_organic = group.columns.get_level_values('Organic').values.astype(bool)
    organics = group.loc[:, is_organic].squeeze()
    non_organics = group.loc[:, ~is_organic].squeeze()
    diff = organics - non_organics
    return diff

def organic_spread_by_region(df):
    """What's the difference between the price of an organic 
    and non-organic avocado within each region?
    """
    return (
        df
        .set_index(['Date', 'Region', 'Organic'])
         ['Weighted Avg Price']
        .unstack(level=['Region', 'Organic'])
        .ffill()
        .groupby(level='Region', axis=1)
        .apply(_organic_spread)
    )

organic_spread_by_region(hass).plot();
plt.gca().set_title("Daily Regional Organic Spread");
plt.legend(bbox_to_anchor=(1, 1));

spread_correlation = organic_spread_by_region(hass).corr()
spread_correlation

import seaborn as sns
grid = sns.clustermap(spread_correlation, annot=True)
fig = grid.fig
axes = fig.axes
ax = axes[2]
ax.set_xticklabels(ax.get_xticklabels(), rotation=45);

"""# Pandas Review

- Pandas extends numpy with more complex datastructures and algorithms.
- If you understand numpy, you understand 90% of pandas.
- `groupby`, `set_index`, and `unstack` are powerful tools for working with categorical data.
- Avocado prices are surprisingly interesting :)

# Thanks!
"""